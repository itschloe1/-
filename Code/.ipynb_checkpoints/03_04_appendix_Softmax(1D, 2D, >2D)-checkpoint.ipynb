{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 차원에도 적용할 수 있는 softmax 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#소프트맥스\n",
    "#소프트맥스는 overflow 문제로 p93에서의 개선된 수식으로 구현한다.\n",
    "#입력 신호 중 최댓값을 빼주면서 inf 해결\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우선 softmax를 구현하는 간단한 식은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99954600e-01,   2.06106005e-09,   4.53978686e-05])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1010,990,1000])\n",
    "c = np.max(x)\n",
    "np.exp(x - c)/np.sum(np.exp(x - c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 방식은 3장에서의 예제를 이용한 softmax function, 1D case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax1d(x):\n",
    "    c = np.max(x)\n",
    "    y = np.exp(x - c)/np.sum(np.exp(x - c))\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교제에서 제공되는 2dimension용 softmax는 아래와 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax2d(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하지만 여기서 두 가지 의문이 있다.\n",
    "###### 1) 왜 dimension이 2인 경우로 한정지었는지\n",
    "##### 2) Transpose를 굳이 쓴 이유는?\n",
    "##### 따라서 나는 \n",
    "##### 1) dimension >= 2인 경우와\n",
    "##### 2) Transpose를 쓰지 않은 경우의 \n",
    "##### softmax2를 생성해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.42482327  0.38473878]\n",
      "  [ 0.14221252  0.3162196 ]\n",
      "  [ 0.17032797  0.03450079]]\n",
      "\n",
      " [[ 0.09736743  0.71473496]\n",
      "  [ 0.77462675  0.40738586]\n",
      "  [ 0.41435051  0.83577028]]\n",
      "\n",
      " [[ 0.04840138  0.2087723 ]\n",
      "  [ 0.72847336  0.83675073]\n",
      "  [ 0.70094989  0.16501865]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension이 3일때, softmax1은 어떻게 동작할까?\n",
    "T = np.random.uniform(size = (3,3,2))\n",
    "print(T)\n",
    "T.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0541539 ,  0.0520261 ],\n",
       "        [ 0.04082192,  0.0485807 ],\n",
       "        [ 0.04198593,  0.03665345]],\n",
       "\n",
       "       [[ 0.0390317 ,  0.07236637],\n",
       "        [ 0.07683294,  0.05321778],\n",
       "        [ 0.05358972,  0.08167737]],\n",
       "\n",
       "       [[ 0.03716651,  0.04363148],\n",
       "        [ 0.07336742,  0.08175749],\n",
       "        [ 0.07137563,  0.04176361]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2d(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0541539 ,  0.0520261 ],\n",
       "        [ 0.04082192,  0.0485807 ],\n",
       "        [ 0.04198593,  0.03665345]],\n",
       "\n",
       "       [[ 0.0390317 ,  0.07236637],\n",
       "        [ 0.07683294,  0.05321778],\n",
       "        [ 0.05358972,  0.08167737]],\n",
       "\n",
       "       [[ 0.03716651,  0.04363148],\n",
       "        [ 0.07336742,  0.08175749],\n",
       "        [ 0.07137563,  0.04176361]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax1d(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내가 정의한 것과 같이 1d , 2d 결과는 같게 나온다. 하지만 이 결과가 맞는지 확인해보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교재에서 주어진 softmax2d의 경우에는 Transpose를 이용하여 벡터끼리의 계산을 진행한 것으로 보여진다. 하지만 나는 transpose를 이용하지 않고 axis를 이용하여 계산을 해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42482327,  0.38473878],\n",
       "       [ 0.77462675,  0.83577028],\n",
       "       [ 0.72847336,  0.83675073]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3d에서 axis = 1을 기준으로 max를 산출하면 다음과 같은 결과가 나온다. \n",
    "np.max(T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42482327,  0.3162196 ,  0.17032797],\n",
       "       [ 0.71473496,  0.77462675,  0.83577028],\n",
       "       [ 0.2087723 ,  0.83675073,  0.70094989]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(T, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### axis = 2일때가 우리가 원하는 값이 나온다. 결국 axis = dimension - 1로 보여진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmaxnd(x):\n",
    "    x = x - np.expand_dims(np.max(x, axis=x.ndim-1),x.ndim-1)\n",
    "    #처음에는 reshape를 써봤으나, n차원인 경우에서 표현을 할 때에 더 복잡할 것 같아 expand_dims로 교체\n",
    "    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=x.ndim-1),x.ndim-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = softmaxnd(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y)  #9개 쌍이 나오니 1.0*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.51001978  0.48998022]\n",
      "  [ 0.45660766  0.54339234]\n",
      "  [ 0.53390469  0.46609531]]\n",
      "\n",
      " [[ 0.3503804   0.6496196 ]\n",
      "  [ 0.59079211  0.40920789]\n",
      "  [ 0.39617706  0.60382294]]\n",
      "\n",
      " [[ 0.45999298  0.54000702]\n",
      "  [ 0.47295707  0.52704293]\n",
      "  [ 0.63086541  0.36913459]]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2d 에다가도 적용시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(size = (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64178713,  0.35821287],\n",
       "       [ 0.37811807,  0.62188193]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxnd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55468694,  0.44531306],\n",
       "       [ 0.58275625,  0.41724375]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D input을 'softmaxnd', 'softmax2d'에 적용하였을 때, 값이 동일하게 나온다.  softmaxnd 로 여러 차원의 softmax를 구현할 수 있다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
