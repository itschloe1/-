{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실 함수(Loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) MSE(Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E = 0.5*(sum*(y_i - t_i)^2)\n",
    "# 실제 값과 예측 값의 오차의 제곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "#제곱의 형태를 나타날 때에 ^2 는 작동하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097500000000000031"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y), np.array(t))\n",
    "#list 였던 y와 i를 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Cross Entropy Error(CEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E = -sum(t_i*log(y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(0.001,1,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e45f4e0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b045438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "#delta를 더해주는 이유는 log(0)이 산출이 되지 않기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCE와 CEE 모두 loss function인데 각각 어느 경우에 쓰이는지? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are suited to different purposes. \n",
    "Mean squared error is appropriate to regression (line/curve fitting) \n",
    "where the goal is to minimize the mean squared error \n",
    "between the training set (points) and the fitted curve. \n",
    "Cross entropy cost is appropriate to classification \n",
    "where the goal is to minimize the number of mis-classified training samples \n",
    "by imposing an exponentially increasing error the closer an output comes \n",
    "to being \"1\" when it should be \"0\", and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Set(N개)마다의 학습에 대한 손실 함수 값들의 합도 산출해야 한다\n",
    "E = -1/N(sum(sum(t_nk)*log(y_nk)))\n",
    "*N개로 나눔으로써 '평균 손실 함수' 산출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 훈련 데이터가 어마어마한 수라면 일부만 골라 학습을 수행하는 방법을 쓰기도 함. 이를 미니배치라고 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random하게 10장만 빼내기 \n",
    "# np.random.choice()\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 batch_mask는 train 안에서 10개를 뽑아온다. 예를 들어 1,3,5,7,9,11,13,15,17,19라는 인덱스를 가져와서 이를 x_train과 t_train에 적용하니, x_batch와 t_batch는 같은 인덱스에 대한 값들이 뽑히게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50734 15641 32496 18395 45642 33204 52538  7517 50013 10916]\n"
     ]
    }
   ],
   "source": [
    "#아래와 같이 batch_mask를 프린트했을 때, 인덱스 값이 지정되어 있는 것을 볼 수 있다. \n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미니배치에 cross entropy error 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t가 원-핫 인코딩일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:                  #else?\n",
    "        t = t.reshape(1, t.size)     #원래 형태로 다시 reshape\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t가 원-핫 인코딩이 아닐 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 왜 '정확도' 대신 '손실 함수의 값'을 이용하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. \n",
    "이때 매개변수의 미분을 계산하고, 그 미분을 단서로 매개변수를 서서히 갱신하는 과정을 반복한다. \n",
    "정확도를 지표로 삼는다면 매개변수의 미분이 대부분에서 0이 되기 때문에 신경망 학습에서는 정확도를 지표로 삼아서는 안 된다. 정확도는 매개변수의 미미한 변화에는 거의 반응을 보이지 않다가, 갑자기 변화하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#미분, 편미분 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기울기(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 변수의 편미분을 벡터로 정리한 것을 '기울기'라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사 하강법(gradient descent method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울어진 방향으로 이동하며, 최솟값을 찾는 것이 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초깃값에서 learning rate * 미분값을 빼주면서 최솟값을 찾아나가는 과정임.\n",
    "learning rate가 작을수록 정밀하게 최솟값을 검토하겠지만 이는 학습속도가 느려진다는 단점도 가지고 있음. \n",
    "신경망 학습에서는 보통 이 learning rate를 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradient_2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cf624f54f315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradient_2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradient_2d'"
     ]
    }
   ],
   "source": [
    "#경사법을 사용한 갱신 과정\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient   #it's not working\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a46c892c4060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 부모 디렉터리의 파일을 가져올 수 있도록 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
